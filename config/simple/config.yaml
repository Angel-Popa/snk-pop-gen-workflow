cluster:
  mkdir -p logs/slurm/{rule} &&
  sbatch
    --partition={resources.partition}
    --time={resources.time}
    --qos={resources.qos}
    --cpus-per-task={threads}
    --mem={resources.mem_gb}
    --job-name=smk-{rule}-{wildcards}
    --output=logs/slurm/{rule}/{rule}-{wildcards}-%j.out
    --parsable # Required to pass job IDs to scancel
default-resources:
  - partition=h24
  - qos=normal
  - mem_gb=100
  - time="04:00:00"
restart-times: 3
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 600
jobs: 100
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True # Required to run with local conda enviroment
cluster-status: status-sacct.sh # Required to monitor the status of the submitted jobs
cluster-cancel: scancel # Required to cancel the jobs with Ctrl + C 
cluster-cancel-nargs: 50